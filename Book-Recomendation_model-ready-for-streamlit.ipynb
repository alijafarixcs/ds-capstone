import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import missingno as msno
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer


#!pip install missingno
#!pip install wordcloud



from sklearn.metrics import mean_squared_error
from math import sqrt

# load data
book_data = pd.read_csv('data/prepared_bookdata.csv')

book_data.head()

drop_columns = ['combined_text']
for col in drop_columns:
    if col in book_data.columns:
        book_data.drop([col], axis=1, inplace=True)

book_data.head()

# Combine relevant text fields into a single text input
book_data['combined_text'] = book_data.apply(lambda row: ' '.join([
    str(row['Title']),
    str(row['description']),
    str(row['authors']),
    str(row['publisher']),
    str(row['publishedDate']),
    str(row['categories'])
]), axis=1)

book_data.head()

book_data.describe()

book_data.isnull().sum()

# Create a mapping from book titles to their indices
indices = pd.Series(book_data.index, index=book_data['Title']).drop_duplicates()

# Reduce book_data by 50%
#book_data_reduced = book_data.sample(frac=0.5, random_state=42)
# Create a new mapping from book titles to their indices for the reduced dataset
#indices_reduced = pd.Series(book_data_reduced.index, index=book_data_reduced['Title']).drop_duplicates()

# ### Preprocessing: Converting the data into numerical vectors ###

# Create a TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')
# Transform documents into TF-IDF matrix
tfidf_matrix = tfidf.fit_transform(book_data['combined_text'])

tfidf_matrix.shape

# ### Computing similarities between items(books) using Cosine-Similarity ###

def calculate_similarity_on_demand(idx, tfidf_matrix):
    # Compute the cosine similarity of the book with index idx to all other books
    cosine_similarities = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()
    return cosine_similarities

# ### Defining the recommendation function to fetch similar books ###

def get_recommendations_on_demand(title, n_recommendations=10):
    # Get the index of the book that matches the title
    idx = indices[title]
    # Calculate similarities on-demand
    sim_scores = calculate_similarity_on_demand(idx, tfidf_matrix)
    # Get the pairwise similarity scores of all books with that book
    sim_scores = list(enumerate(sim_scores))
    # Sort the books based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # Get the scores of the n most similar books
    sim_scores = sim_scores[1:n_recommendations+1]
    # Get the book indices
    book_indices = [i[0] for i in sim_scores]
    # Return the top n most similar books
    return book_data[['Title', 'authors', 'categories', 'rating', 'ratingsCount']].iloc[book_indices]

# Example
example_title = book_data[['Title', 'authors', 'categories']].loc[1]
example_title

# Example usage
example_title = book_data['Title'].iloc[1]
recommendations = get_recommendations_on_demand(example_title)
recommendations

# ### The Evaluation Part ###

# ### Predict ratings for each user-book pair based on similarities ###

# Create a mapping from book titles to indices
indices = pd.Series(book_data.index, index=book_data['Title']).drop_duplicates()

# Compute cosine similarity between items
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Function to get the predicted rating of a book for a user
def predict_rating(user_id, book_index, user_ratings, sim_matrix):
    sim_scores = sim_matrix[book_index]
    user_rated_indices = user_ratings[user_ratings['User_id'] == user_id].index
    valid_indices = [i for i in user_rated_indices if i < len(sim_scores)]
    user_ratings_values = user_ratings.loc[valid_indices, 'rating'].values
    if len(user_ratings_values) == 0:
        return np.mean(user_ratings['rating'])  # Return the global average if no ratings
    relevant_sim_scores = sim_scores[valid_indices]
    if np.sum(relevant_sim_scores) == 0:
        return np.mean(user_ratings['rating'])  # Return the global average if no similarities
    weighted_sum = np.dot(relevant_sim_scores, user_ratings_values)
    sum_of_sim_scores = np.sum(relevant_sim_scores)
    return weighted_sum / sum_of_sim_scores

# Generate predictions for all user-item pairs
user_ids = book_data['User_id'].unique()
predictions = []
for user_id in user_ids:
    user_ratings = book_data[book_data['User_id'] == user_id]
    for index, row in user_ratings.iterrows():
        if row['Title'] not in indices:
            continue
        book_index = indices[row['Title']]
        predicted_rating = predict_rating(user_id, book_index, book_data, cosine_sim)
        predictions.append((user_id, row['Title'], predicted_rating, row['rating']))

# Create a DataFrame with predictions and actual ratings
pred_df = pd.DataFrame(predictions, columns=['User_id', 'Title', 'Predicted_Rating', 'Actual_Rating'])
pred_df

# Calculate RMSE
rmse = sqrt(mean_squared_error(pred_df['Actual_Rating'], pred_df['Predicted_Rating']))
rmse









